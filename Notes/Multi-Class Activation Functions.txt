Multi-Class Activation Functions

Multi-class activation functions take on a specialized role in helping models determine the probability that of a given
input beloging to a specific class. The easiest way to organize multiple classes is to have one output node per class.

There are 2 main types of multi-class situations:
    - Non-Exclusive Classes
        A data point can have multiple classes/categories assigned to it.
        EX: Photos can have multiple tags like beach, family, vacation, etc.
    - Mutually Exclusive Classes
        Only one class per data point.
        EX: Photos can be categorized as being grayscale or full color but not both at the same time.

To organize multiple classes, you must organize categories for the output layer.

Non-Exclusive Function
    - Sigmoid Function
        Each neuron will output a value between 0 and 1, indicating the probability of having that class assigned to it.
        It is a non-linear activation function.
    - SoftPlus Function
        Ensures that the output is always positive and differentiable at all points which is an advantage over the traditional ReLU function.
        The function outputs values in the range 0 to positive infinity, similar to ReLU but without the hard 0 threshold.
        SoftPlus is a smooth, continuous function, meaning it avoids the sharp discontinuities of ReLU which can sometimes lead to problems
        during optimization.
        It is a non-linear activation function.

Mutually Exclusive Function
    - Softmax Function
        It transforms raw output scores from a neural network into probabilites. It works by squashing the output values of each class into the
        range of 0 to 1 while ensuring that the sum of all probabilities equals 1.
        Softmax function ensures that each clas sis assigned a probability, helping to identify which class the input belongs to.
        It is a non-linear activation function.

